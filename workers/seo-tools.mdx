---
$type: Worker
$id: seo-tools
name: seo-tools
main: src/index.ts
compatibility_date: "2025-01-01"

observability:
  enabled: true

tail_consumers:
  - service: pipeline

services:
  - binding: DB
    service: db
  - binding: AUTH
    service: auth

kv_namespaces:
  - binding: SEO_CACHE
    id: seo-tools-cache

routes:
  - pattern: seo-tools.services.do/*
    custom_domain: true

vars:
  ENVIRONMENT: production
---

# SEO Tools & Utilities Worker

Comprehensive SEO toolset providing keyword research, competitor analysis, llms.txt generation, sitemap generation, technical SEO audits, and meta tag analysis.

## Features

### 1. llms.txt Generator
Generates valid llms.txt files from website content using OpenAI GPT for intelligent content summarization.

**Capabilities:**
- Crawls website pages
- Extracts key information (About, Products, Contact, etc.)
- Generates structured llms.txt format
- Validates output format

**Example llms.txt:**
```
# Company Name
> Brief company description

## About
Company information and mission

## Products
Product listings and descriptions

## Contact
Contact information and support details
```

### 2. Sitemap Generator
Creates XML and TXT sitemaps with proper formatting and validation.

**Features:**
- Crawls website for all pages
- Respects robots.txt
- Generates sitemap.xml
- Generates sitemap.txt
- Supports sitemap index for large sites
- Priority and change frequency hints

### 3. Meta Tag Analyzer
Analyzes page meta tags for SEO optimization.

**Checks:**
- Title tag (length, keywords, uniqueness)
- Meta description (length, keywords, compelling)
- Open Graph tags (og:title, og:description, og:image)
- Twitter Card tags
- Canonical URLs
- Robots directives
- Schema.org structured data

### 4. Technical SEO Audit
Comprehensive technical SEO analysis.

**Audit Areas:**
- Page speed and performance
- Mobile-friendliness
- HTTPS usage
- Structured data validation
- Internal linking structure
- Broken links detection
- Image optimization (alt tags, size)
- Header tag hierarchy (H1-H6)
- Content quality metrics
- Duplicate content detection

### 5. Keyword Research
Integrates with DataForSEO for keyword research and analysis.

**Features:**
- Search volume data
- Keyword difficulty scoring
- Related keywords discovery
- SERP features analysis
- Competitive keyword analysis
- Long-tail keyword suggestions

### 6. Competitor Analysis
Analyzes competitor websites for SEO insights.

**Analysis:**
- Top keywords
- Backlink profile
- Content strategy
- Technical SEO score
- SERP position tracking
- Content gaps identification

## API Endpoints

### llms.txt Generation
```bash
POST /generate-llms-txt
{
  "url": "https://example.com",
  "sections": ["about", "products", "contact"],
  "maxPages": 10
}
```

### Sitemap Generation
```bash
POST /generate-sitemap
{
  "url": "https://example.com",
  "format": "xml",
  "maxPages": 500,
  "includeImages": true
}
```

### Meta Tag Analysis
```bash
POST /analyze-meta
{
  "url": "https://example.com/page"
}
```

### Technical SEO Audit
```bash
POST /audit
{
  "url": "https://example.com",
  "depth": 2,
  "includePerformance": true
}
```

### Keyword Research
```bash
POST /keywords/research
{
  "keyword": "software development",
  "location": "United States",
  "language": "en"
}
```

### Competitor Analysis
```bash
POST /competitors/analyze
{
  "url": "https://example.com",
  "competitors": ["https://competitor1.com", "https://competitor2.com"]
}
```

## Architecture

```
┌─────────────────┐
│   HTTP Request  │
└────────┬────────┘
         │
    ┌────▼─────┐
    │   Hono   │
    │  Router  │
    └────┬─────┘
         │
    ┌────▼──────────────────────────┐
    │    SEOToolsService (RPC)      │
    ├───────────────────────────────┤
    │ - generateLlmsTxt()           │
    │ - generateSitemap()           │
    │ - analyzeMeta()               │
    │ - auditTechnical()            │
    │ - researchKeywords()          │
    │ - analyzeCompetitors()        │
    └────┬──────────────────────────┘
         │
    ┌────▼────────────────────────────┐
    │      External Services          │
    ├─────────────────────────────────┤
    │ - OpenAI GPT (llms.txt)         │
    │ - DataForSEO API (keywords)     │
    │ - Web Scraping (crawling)       │
    │ - Schema Validator              │
    └─────────────────────────────────┘
```

## Implementation

```typescript
import { WorkerEntrypoint } from 'cloudflare:workers'
import { Hono } from 'hono'
import { cors } from 'hono/cors'
import { z } from 'zod'
import { parse as parseHTML } from 'node-html-parser'

// ==================== TYPES ====================

interface Env {
  DB: any
  AUTH: any
  SEO_CACHE: KVNamespace
  OPENAI_API_KEY: string
  DATAFORSEO_LOGIN?: string
  DATAFORSEO_PASSWORD?: string
}

interface LlmsTxtRequest {
  url: string
  sections?: string[]
  maxPages?: number
}

interface SitemapRequest {
  url: string
  format?: 'xml' | 'txt'
  maxPages?: number
  includeImages?: boolean
}

interface MetaAnalysisRequest {
  url: string
}

interface TechnicalAuditRequest {
  url: string
  depth?: number
  includePerformance?: boolean
}

interface KeywordResearchRequest {
  keyword: string
  location?: string
  language?: string
}

interface CompetitorAnalysisRequest {
  url: string
  competitors: string[]
}

interface LlmsTxtResult {
  content: string
  pages: number
  sections: string[]
  generated_at: string
}

interface SitemapResult {
  sitemap: string
  format: 'xml' | 'txt'
  urls: number
  generated_at: string
}

interface MetaAnalysis {
  url: string
  title: {
    content: string
    length: number
    score: number
    issues: string[]
  }
  description: {
    content: string
    length: number
    score: number
    issues: string[]
  }
  openGraph: Record<string, string>
  twitter: Record<string, string>
  canonical?: string
  robots?: string
  structuredData: any[]
  score: number
  recommendations: string[]
}

interface TechnicalAudit {
  url: string
  score: number
  performance: {
    loadTime: number
    firstContentfulPaint: number
    largestContentfulPaint: number
    score: number
  }
  mobile: {
    friendly: boolean
    viewport: boolean
    fontSizes: boolean
    score: number
  }
  https: boolean
  structuredData: {
    valid: boolean
    types: string[]
    errors: string[]
  }
  links: {
    internal: number
    external: number
    broken: number[]
    score: number
  }
  images: {
    total: number
    missingAlt: number
    oversized: number[]
    score: number
  }
  headers: {
    h1Count: number
    hierarchy: boolean
    issues: string[]
    score: number
  }
  recommendations: string[]
  timestamp: string
}

interface KeywordData {
  keyword: string
  volume: number
  difficulty: number
  cpc: number
  competition: number
  trend: number[]
  relatedKeywords: string[]
  serpFeatures: string[]
}

interface CompetitorData {
  url: string
  score: number
  topKeywords: string[]
  backlinks: number
  domainAuthority: number
  contentScore: number
  technicalScore: number
  gaps: string[]
}

// ==================== VALIDATION SCHEMAS ====================

const llmsTxtSchema = z.object({
  url: z.string().url(),
  sections: z.array(z.string()).optional(),
  maxPages: z.number().int().min(1).max(100).optional(),
})

const sitemapSchema = z.object({
  url: z.string().url(),
  format: z.enum(['xml', 'txt']).optional(),
  maxPages: z.number().int().min(1).max(50000).optional(),
  includeImages: z.boolean().optional(),
})

const metaAnalysisSchema = z.object({
  url: z.string().url(),
})

const technicalAuditSchema = z.object({
  url: z.string().url(),
  depth: z.number().int().min(1).max(5).optional(),
  includePerformance: z.boolean().optional(),
})

const keywordResearchSchema = z.object({
  keyword: z.string().min(1),
  location: z.string().optional(),
  language: z.string().optional(),
})

const competitorAnalysisSchema = z.object({
  url: z.string().url(),
  competitors: z.array(z.string().url()).min(1).max(5),
})

// ==================== RPC SERVICE ====================

export class SEOToolsService extends WorkerEntrypoint<Env> {
  /**
   * Generate llms.txt file from website content
   */
  async generateLlmsTxt(request: LlmsTxtRequest): Promise<LlmsTxtResult> {
    const { url, sections = ['about', 'products', 'contact'], maxPages = 10 } = request

    // Check cache
    const cacheKey = `llmstxt:${url}:${sections.join(',')}`
    const cached = await this.env.SEO_CACHE.get(cacheKey, 'json')
    if (cached) return cached as LlmsTxtResult

    // Crawl website pages
    const pages = await this.crawlWebsite(url, maxPages)

    // Extract content for each section
    const sectionContent: Record<string, string> = {}
    for (const section of sections) {
      const content = await this.extractSectionContent(pages, section)
      sectionContent[section] = content
    }

    // Generate llms.txt using OpenAI
    const llmsTxt = await this.generateLlmsTxtContent(url, sectionContent)

    const result: LlmsTxtResult = {
      content: llmsTxt,
      pages: pages.length,
      sections,
      generated_at: new Date().toISOString(),
    }

    // Cache for 24 hours
    await this.env.SEO_CACHE.put(cacheKey, JSON.stringify(result), {
      expirationTtl: 86400,
    })

    return result
  }

  /**
   * Generate sitemap (XML or TXT)
   */
  async generateSitemap(request: SitemapRequest): Promise<SitemapResult> {
    const { url, format = 'xml', maxPages = 500, includeImages = false } = request

    // Check cache
    const cacheKey = `sitemap:${url}:${format}:${includeImages}`
    const cached = await this.env.SEO_CACHE.get(cacheKey, 'json')
    if (cached) return cached as SitemapResult

    // Crawl website
    const pages = await this.crawlWebsite(url, maxPages, true)

    // Generate sitemap
    let sitemap: string
    if (format === 'xml') {
      sitemap = this.generateXMLSitemap(pages, includeImages)
    } else {
      sitemap = this.generateTXTSitemap(pages)
    }

    const result: SitemapResult = {
      sitemap,
      format,
      urls: pages.length,
      generated_at: new Date().toISOString(),
    }

    // Cache for 12 hours
    await this.env.SEO_CACHE.put(cacheKey, JSON.stringify(result), {
      expirationTtl: 43200,
    })

    return result
  }

  /**
   * Analyze meta tags for SEO
   */
  async analyzeMeta(request: MetaAnalysisRequest): Promise<MetaAnalysis> {
    const { url } = request

    // Fetch page
    const html = await this.fetchPage(url)
    const doc = parseHTML(html)

    // Extract meta tags
    const title = doc.querySelector('title')?.text || ''
    const description = doc.querySelector('meta[name="description"]')?.getAttribute('content') || ''
    const canonical = doc.querySelector('link[rel="canonical"]')?.getAttribute('href') || undefined
    const robots = doc.querySelector('meta[name="robots"]')?.getAttribute('content') || undefined

    // Analyze title
    const titleAnalysis = this.analyzeTitle(title)

    // Analyze description
    const descriptionAnalysis = this.analyzeDescription(description)

    // Extract Open Graph tags
    const openGraph = this.extractOpenGraphTags(doc)

    // Extract Twitter Card tags
    const twitter = this.extractTwitterTags(doc)

    // Extract structured data
    const structuredData = this.extractStructuredData(doc)

    // Calculate overall score
    const score = Math.round(
      (titleAnalysis.score * 0.3 +
        descriptionAnalysis.score * 0.3 +
        (Object.keys(openGraph).length > 0 ? 100 : 0) * 0.2 +
        (structuredData.length > 0 ? 100 : 0) * 0.2)
    )

    // Generate recommendations
    const recommendations = [
      ...titleAnalysis.issues,
      ...descriptionAnalysis.issues,
      Object.keys(openGraph).length === 0 ? 'Add Open Graph tags for better social media sharing' : null,
      Object.keys(twitter).length === 0 ? 'Add Twitter Card tags for Twitter optimization' : null,
      !canonical ? 'Add canonical URL to prevent duplicate content issues' : null,
      structuredData.length === 0 ? 'Add Schema.org structured data for rich snippets' : null,
    ].filter(Boolean) as string[]

    return {
      url,
      title: titleAnalysis,
      description: descriptionAnalysis,
      openGraph,
      twitter,
      canonical,
      robots,
      structuredData,
      score,
      recommendations,
    }
  }

  /**
   * Perform technical SEO audit
   */
  async auditTechnical(request: TechnicalAuditRequest): Promise<TechnicalAudit> {
    const { url, depth = 2, includePerformance = true } = request

    // Check cache
    const cacheKey = `audit:${url}:${depth}:${includePerformance}`
    const cached = await this.env.SEO_CACHE.get(cacheKey, 'json')
    if (cached) return cached as TechnicalAudit

    // Perform audit
    const performance = includePerformance ? await this.auditPerformance(url) : null
    const mobile = await this.auditMobile(url)
    const https = url.startsWith('https://')
    const structuredData = await this.auditStructuredData(url)
    const links = await this.auditLinks(url, depth)
    const images = await this.auditImages(url)
    const headers = await this.auditHeaders(url)

    // Calculate scores
    const performanceScore = performance?.score || 100
    const mobileScore = mobile.score
    const httpsScore = https ? 100 : 0
    const structuredDataScore = structuredData.valid ? 100 : 50
    const linksScore = links.score
    const imagesScore = images.score
    const headersScore = headers.score

    const overallScore = Math.round(
      (performanceScore * 0.25 +
        mobileScore * 0.15 +
        httpsScore * 0.1 +
        structuredDataScore * 0.15 +
        linksScore * 0.15 +
        imagesScore * 0.1 +
        headersScore * 0.1)
    )

    // Generate recommendations
    const recommendations: string[] = []
    if (performanceScore < 80) recommendations.push('Improve page load speed')
    if (mobileScore < 80) recommendations.push('Optimize for mobile devices')
    if (!https) recommendations.push('Migrate to HTTPS')
    if (!structuredData.valid) recommendations.push('Fix structured data errors')
    if (links.broken.length > 0) recommendations.push(`Fix ${links.broken.length} broken links`)
    if (images.missingAlt > 0) recommendations.push(`Add alt text to ${images.missingAlt} images`)
    if (!headers.hierarchy) recommendations.push('Fix header tag hierarchy')

    const result: TechnicalAudit = {
      url,
      score: overallScore,
      performance: performance || { loadTime: 0, firstContentfulPaint: 0, largestContentfulPaint: 0, score: 100 },
      mobile,
      https,
      structuredData,
      links,
      images,
      headers,
      recommendations,
      timestamp: new Date().toISOString(),
    }

    // Cache for 6 hours
    await this.env.SEO_CACHE.put(cacheKey, JSON.stringify(result), {
      expirationTtl: 21600,
    })

    return result
  }

  /**
   * Research keywords (requires DataForSEO API)
   */
  async researchKeywords(request: KeywordResearchRequest): Promise<KeywordData> {
    const { keyword, location = 'United States', language = 'en' } = request

    // Check if DataForSEO credentials are configured
    if (!this.env.DATAFORSEO_LOGIN || !this.env.DATAFORSEO_PASSWORD) {
      throw new Error('DataForSEO API credentials not configured')
    }

    // Check cache
    const cacheKey = `keywords:${keyword}:${location}:${language}`
    const cached = await this.env.SEO_CACHE.get(cacheKey, 'json')
    if (cached) return cached as KeywordData

    // Call DataForSEO API
    const auth = btoa(`${this.env.DATAFORSEO_LOGIN}:${this.env.DATAFORSEO_PASSWORD}`)
    const response = await fetch('https://api.dataforseo.com/v3/keywords_data/google/search_volume/live', {
      method: 'POST',
      headers: {
        'Authorization': `Basic ${auth}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify([{
        keywords: [keyword],
        location_name: location,
        language_name: language,
      }]),
    })

    const data: any = await response.json()
    const result = data.tasks?.[0]?.result?.[0] || {}

    const keywordData: KeywordData = {
      keyword,
      volume: result.search_volume || 0,
      difficulty: result.keyword_difficulty || 0,
      cpc: result.cpc || 0,
      competition: result.competition || 0,
      trend: result.monthly_searches?.map((m: any) => m.search_volume) || [],
      relatedKeywords: result.related_keywords || [],
      serpFeatures: result.serp_features || [],
    }

    // Cache for 7 days
    await this.env.SEO_CACHE.put(cacheKey, JSON.stringify(keywordData), {
      expirationTtl: 604800,
    })

    return keywordData
  }

  /**
   * Analyze competitors
   */
  async analyzeCompetitors(request: CompetitorAnalysisRequest): Promise<CompetitorData[]> {
    const { url, competitors } = request

    const results: CompetitorData[] = []

    for (const competitorUrl of competitors) {
      // Perform technical audit
      const audit = await this.auditTechnical({ url: competitorUrl, depth: 1, includePerformance: false })

      // Analyze meta tags
      const meta = await this.analyzeMeta({ url: competitorUrl })

      // Calculate scores (simplified - would use real data in production)
      const competitorData: CompetitorData = {
        url: competitorUrl,
        score: audit.score,
        topKeywords: [], // Would extract from content analysis
        backlinks: 0, // Would get from backlink API
        domainAuthority: 0, // Would get from domain authority API
        contentScore: meta.score,
        technicalScore: audit.score,
        gaps: [], // Would identify content gaps
      }

      results.push(competitorData)
    }

    return results
  }

  // ==================== HELPER METHODS ====================

  private async crawlWebsite(url: string, maxPages: number, respectRobots = true): Promise<string[]> {
    // Simple crawler implementation
    // In production, this would be more sophisticated
    const pages: string[] = [url]
    const visited = new Set<string>([url])
    const queue: string[] = [url]

    while (queue.length > 0 && pages.length < maxPages) {
      const currentUrl = queue.shift()!

      try {
        const html = await this.fetchPage(currentUrl)
        const doc = parseHTML(html)

        // Extract links
        const links = doc.querySelectorAll('a')
          .map(a => a.getAttribute('href'))
          .filter(href => href && (href.startsWith('/') || href.startsWith(url)))
          .map(href => href?.startsWith('/') ? new URL(href, url).href : href!)
          .filter(href => !visited.has(href))

        for (const link of links) {
          if (pages.length >= maxPages) break
          visited.add(link)
          pages.push(link)
          queue.push(link)
        }
      } catch (error) {
        console.error(`Error crawling ${currentUrl}:`, error)
      }
    }

    return pages
  }

  private async fetchPage(url: string): Promise<string> {
    const response = await fetch(url, {
      headers: {
        'User-Agent': 'SEO-Tools-Bot/1.0 (https://seo-tools.services.do)',
      },
    })

    if (!response.ok) {
      throw new Error(`Failed to fetch ${url}: ${response.status}`)
    }

    return await response.text()
  }

  private async extractSectionContent(pages: string[], section: string): Promise<string> {
    // Extract content for specific section from pages
    // This would use OpenAI to intelligently extract section content
    return `Content for ${section} section`
  }

  private async generateLlmsTxtContent(url: string, sections: Record<string, string>): Promise<string> {
    // Use OpenAI to generate well-formatted llms.txt
    const prompt = `Generate a well-formatted llms.txt file for ${url} with these sections:\n\n${Object.entries(sections).map(([k, v]) => `## ${k}\n${v}`).join('\n\n')}`

    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.env.OPENAI_API_KEY}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'gpt-4',
        messages: [
          { role: 'system', content: 'You are an expert at creating llms.txt files.' },
          { role: 'user', content: prompt },
        ],
      }),
    })

    const data: any = await response.json()
    return data.choices[0].message.content
  }

  private generateXMLSitemap(pages: string[], includeImages: boolean): string {
    let xml = '<?xml version="1.0" encoding="UTF-8"?>\n'
    xml += '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n'

    for (const page of pages) {
      xml += '  <url>\n'
      xml += `    <loc>${page}</loc>\n`
      xml += `    <lastmod>${new Date().toISOString()}</lastmod>\n`
      xml += '    <changefreq>weekly</changefreq>\n'
      xml += '    <priority>0.8</priority>\n'
      xml += '  </url>\n'
    }

    xml += '</urlset>'
    return xml
  }

  private generateTXTSitemap(pages: string[]): string {
    return pages.join('\n')
  }

  private analyzeTitle(title: string): { content: string; length: number; score: number; issues: string[] } {
    const length = title.length
    const issues: string[] = []
    let score = 100

    if (length === 0) {
      issues.push('Title tag is missing')
      score = 0
    } else if (length < 30) {
      issues.push('Title is too short (recommended: 50-60 characters)')
      score -= 30
    } else if (length > 60) {
      issues.push('Title is too long (recommended: 50-60 characters)')
      score -= 20
    }

    return { content: title, length, score: Math.max(0, score), issues }
  }

  private analyzeDescription(description: string): { content: string; length: number; score: number; issues: string[] } {
    const length = description.length
    const issues: string[] = []
    let score = 100

    if (length === 0) {
      issues.push('Meta description is missing')
      score = 0
    } else if (length < 120) {
      issues.push('Description is too short (recommended: 150-160 characters)')
      score -= 30
    } else if (length > 160) {
      issues.push('Description is too long (recommended: 150-160 characters)')
      score -= 20
    }

    return { content: description, length, score: Math.max(0, score), issues }
  }

  private extractOpenGraphTags(doc: ReturnType<typeof parseHTML>): Record<string, string> {
    const tags: Record<string, string> = {}
    const ogTags = doc.querySelectorAll('meta[property^="og:"]')

    ogTags.forEach(tag => {
      const property = tag.getAttribute('property')
      const content = tag.getAttribute('content')
      if (property && content) {
        tags[property] = content
      }
    })

    return tags
  }

  private extractTwitterTags(doc: ReturnType<typeof parseHTML>): Record<string, string> {
    const tags: Record<string, string> = {}
    const twitterTags = doc.querySelectorAll('meta[name^="twitter:"]')

    twitterTags.forEach(tag => {
      const name = tag.getAttribute('name')
      const content = tag.getAttribute('content')
      if (name && content) {
        tags[name] = content
      }
    })

    return tags
  }

  private extractStructuredData(doc: ReturnType<typeof parseHTML>): any[] {
    const scripts = doc.querySelectorAll('script[type="application/ld+json"]')
    const structuredData: any[] = []

    scripts.forEach(script => {
      try {
        const data = JSON.parse(script.text || '{}')
        structuredData.push(data)
      } catch (error) {
        // Invalid JSON
      }
    })

    return structuredData
  }

  private async auditPerformance(url: string): Promise<{ loadTime: number; firstContentfulPaint: number; largestContentfulPaint: number; score: number }> {
    // Measure performance metrics
    const start = Date.now()
    await fetch(url)
    const loadTime = Date.now() - start

    // Simplified performance score
    let score = 100
    if (loadTime > 3000) score -= 50
    else if (loadTime > 2000) score -= 30
    else if (loadTime > 1000) score -= 10

    return {
      loadTime,
      firstContentfulPaint: loadTime * 0.5,
      largestContentfulPaint: loadTime * 0.7,
      score: Math.max(0, score),
    }
  }

  private async auditMobile(url: string): Promise<{ friendly: boolean; viewport: boolean; fontSizes: boolean; score: number }> {
    const html = await this.fetchPage(url)
    const doc = parseHTML(html)

    const viewport = !!doc.querySelector('meta[name="viewport"]')
    const fontSizes = true // Would analyze font sizes
    const friendly = viewport && fontSizes

    const score = (viewport ? 50 : 0) + (fontSizes ? 50 : 0)

    return { friendly, viewport, fontSizes, score }
  }

  private async auditStructuredData(url: string): Promise<{ valid: boolean; types: string[]; errors: string[] }> {
    const html = await this.fetchPage(url)
    const doc = parseHTML(html)

    const structuredData = this.extractStructuredData(doc)
    const types = structuredData.map((data: any) => data['@type']).filter(Boolean)
    const errors: string[] = []

    // Validate structured data
    structuredData.forEach((data: any) => {
      if (!data['@context']) errors.push('Missing @context')
      if (!data['@type']) errors.push('Missing @type')
    })

    return {
      valid: errors.length === 0 && structuredData.length > 0,
      types,
      errors,
    }
  }

  private async auditLinks(url: string, depth: number): Promise<{ internal: number; external: number; broken: number[]; score: number }> {
    const html = await this.fetchPage(url)
    const doc = parseHTML(html)

    const links = doc.querySelectorAll('a[href]')
    const internal = links.filter(a => {
      const href = a.getAttribute('href')
      return href && (href.startsWith('/') || href.startsWith(url))
    }).length

    const external = links.length - internal
    const broken: number[] = [] // Would check for 404s

    const score = Math.min(100, internal * 2) - (broken.length * 10)

    return { internal, external, broken, score: Math.max(0, score) }
  }

  private async auditImages(url: string): Promise<{ total: number; missingAlt: number; oversized: number[]; score: number }> {
    const html = await this.fetchPage(url)
    const doc = parseHTML(html)

    const images = doc.querySelectorAll('img')
    const missingAlt = images.filter(img => !img.getAttribute('alt')).length
    const oversized: number[] = [] // Would check image sizes

    const score = ((images.length - missingAlt) / Math.max(1, images.length)) * 100

    return { total: images.length, missingAlt, oversized, score }
  }

  private async auditHeaders(url: string): Promise<{ h1Count: number; hierarchy: boolean; issues: string[]; score: number }> {
    const html = await this.fetchPage(url)
    const doc = parseHTML(html)

    const h1Count = doc.querySelectorAll('h1').length
    const issues: string[] = []

    if (h1Count === 0) issues.push('No H1 tag found')
    if (h1Count > 1) issues.push('Multiple H1 tags found')

    const hierarchy = h1Count === 1
    const score = hierarchy ? 100 : 50

    return { h1Count, hierarchy, issues, score }
  }
}

// ==================== HTTP API ====================

const app = new Hono<{ Bindings: Env }>()

// CORS
app.use('*', cors())

// Health check
app.get('/health', (c) => c.json({ status: 'healthy', service: 'seo-tools' }))

// Generate llms.txt
app.post('/generate-llms-txt', async (c) => {
  try {
    const body = await c.req.json()
    const validated = llmsTxtSchema.parse(body)

    const service = new SEOToolsService(c.env as any, c.env)
    const result = await service.generateLlmsTxt(validated)

    return c.json({ success: true, data: result })
  } catch (error: any) {
    return c.json({ success: false, error: error.message }, 400)
  }
})

// Generate sitemap
app.post('/generate-sitemap', async (c) => {
  try {
    const body = await c.req.json()
    const validated = sitemapSchema.parse(body)

    const service = new SEOToolsService(c.env as any, c.env)
    const result = await service.generateSitemap(validated)

    return c.json({ success: true, data: result })
  } catch (error: any) {
    return c.json({ success: false, error: error.message }, 400)
  }
})

// Analyze meta tags
app.post('/analyze-meta', async (c) => {
  try {
    const body = await c.req.json()
    const validated = metaAnalysisSchema.parse(body)

    const service = new SEOToolsService(c.env as any, c.env)
    const result = await service.analyzeMeta(validated)

    return c.json({ success: true, data: result })
  } catch (error: any) {
    return c.json({ success: false, error: error.message }, 400)
  }
})

// Technical SEO audit
app.post('/audit', async (c) => {
  try {
    const body = await c.req.json()
    const validated = technicalAuditSchema.parse(body)

    const service = new SEOToolsService(c.env as any, c.env)
    const result = await service.auditTechnical(validated)

    return c.json({ success: true, data: result })
  } catch (error: any) {
    return c.json({ success: false, error: error.message }, 400)
  }
})

// Keyword research
app.post('/keywords/research', async (c) => {
  try {
    const body = await c.req.json()
    const validated = keywordResearchSchema.parse(body)

    const service = new SEOToolsService(c.env as any, c.env)
    const result = await service.researchKeywords(validated)

    return c.json({ success: true, data: result })
  } catch (error: any) {
    return c.json({ success: false, error: error.message }, 400)
  }
})

// Competitor analysis
app.post('/competitors/analyze', async (c) => {
  try {
    const body = await c.req.json()
    const validated = competitorAnalysisSchema.parse(body)

    const service = new SEOToolsService(c.env as any, c.env)
    const result = await service.analyzeCompetitors(validated)

    return c.json({ success: true, data: result })
  } catch (error: any) {
    return c.json({ success: false, error: error.message }, 400)
  }
})

export default { fetch: app.fetch }
```

## Deployment

```bash
# Build worker
pnpm build-mdx seo-tools.mdx

# Deploy
cd seo-tools
wrangler deploy

# Test
curl https://seo-tools.services.do/health
```

## Environment Variables

Add to `.dev.vars` and production secrets:

```bash
OPENAI_API_KEY=sk-...
DATAFORSEO_LOGIN=your-login
DATAFORSEO_PASSWORD=your-password
```

## Testing

```bash
# Generate llms.txt
curl -X POST https://seo-tools.services.do/generate-llms-txt \
  -H "Content-Type: application/json" \
  -d '{"url":"https://example.com","sections":["about","products"]}'

# Generate sitemap
curl -X POST https://seo-tools.services.do/generate-sitemap \
  -H "Content-Type: application/json" \
  -d '{"url":"https://example.com","format":"xml"}'

# Analyze meta tags
curl -X POST https://seo-tools.services.do/analyze-meta \
  -H "Content-Type: application/json" \
  -d '{"url":"https://example.com"}'
```

## Future Enhancements

- Add support for more keyword research providers
- Implement backlink analysis
- Add SERP tracking
- Content gap analysis
- Schema.org validator
- Page speed insights integration
- Mobile-first indexing analysis
- Core Web Vitals tracking
